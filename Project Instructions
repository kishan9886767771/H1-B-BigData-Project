create database h1b;

use h1b;

CREATE TABLE h1b_applications(s_no int,case_status string,
employer_name string, soc_name string, job_title string,
full_time_position string,prevailing_wage bigint,year string, worksite
string, longitute double, latitute double )
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"separatorChar" = ",",
"quoteChar" = "\""
) STORED AS TEXTFILE;

load data local inpath '/home/hduser/Downloads/Project files/h1b.csv' overwrite into table
h1b_applications;

CREATE TABLE h1b_app2(s_no int,case_status string, employer_name
string, soc_name string, job_title string, full_time_position
string,prevailing_wage bigint,year string, worksite string, longitute
double, latitute double )
row format delimited
fields terminated by '\t'
STORED AS TEXTFILE;


INSERT OVERWRITE TABLE h1b_app2 SELECT regexp_replace(s_no, "\t", ""),
regexp_replace(case_status, "\t", ""), regexp_replace(employer_name,
"\t", ""), regexp_replace(soc_name, "\t", ""),
regexp_replace(job_title, "\t", ""),
regexp_replace(full_time_position, "\t", ""), prevailing_wage,
regexp_replace(year, "\t", ""), regexp_replace(worksite, "\t", ""),
regexp_replace(longitute, "\t", ""), regexp_replace(latitute, "\t",
"") FROM h1b_applications where case_status != "NA";

CREATE TABLE h1b_final(s_no int,case_status string, employer_name string, soc_name string, job_title string, full_time_position string,prevailing_wage bigint,year string, worksite string, longitute double, latitute double ) row format delimited fields terminated by '\t' STORED AS TEXTFILE;

INSERT OVERWRITE TABLE h1b_final SELECT s_no,case when trim(case_status) = "PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED" then "DENIED" when trim(case_status) = "REJECTED" then "DENIED" when trim(case_status) = "INVALIDATED" then "DENIED" else case_status end, employer_name, soc_name, job_title, full_time_position, case when prevailing_wage is null then 100000 else prevailing_wage end,year, worksite, longitute, latitute FROM h1b_app2;


select year, count(*) from h1b_final group by year;
--------------------------------------------------
2011    358767
2013    442114
2015    618727
2012    415607
2014    519427
2016    647803

total 3002445 records

select case_status, count(*) from h1b_final group by case_status;
----------------------------------------------------------------

new data
-------
CERTIFIED-WITHDRAWN     202659
WITHDRAWN       89799
CERTIFIED       2615623
DENIED  94364

total records : 3002445


hive to local directory
---------------------------
INSERT OVERWRITE LOCAL DIRECTORY '/home/hduser/h1bproject/h1bdata/' row format delimited fields terminated by '\t' select * from h1b_app2;

Local system to HDFS
----------------------
hadoop fs -put /home/hduser/h1bproject/h1bdata /niit/h1b

Tools to Install
------------------
1)All Hadoop Services ->MapReduce, Hive, Pig, Sqoop etc...
2)GnuPlot -> For Graph
3)Yad -> For Progress Dialogue box
4)Zenity -> For Progress Dialogue box
5)MYSQL

*Extract The H1bProject tar file
*Place it in /home/hduser
*Then Install all the tools
*Do All the steps To Upload csv file to hive 
*Then hive -> local directory -> hdfs
*Then run ./start.sh
